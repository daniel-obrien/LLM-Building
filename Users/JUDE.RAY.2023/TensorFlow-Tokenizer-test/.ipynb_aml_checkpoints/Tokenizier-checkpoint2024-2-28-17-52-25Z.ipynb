{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()\n",
        "\n",
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "\n",
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))\n",
        "\n",
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)\n",
        "\n",
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")\n",
        "\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n",
        "\n",
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])\n",
        "\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)\n",
        "\n",
        "write_vocab_file('pt_vocab.txt', pt_vocab)\n",
        "\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n",
        "\n",
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])\n",
        "\n",
        "write_vocab_file('en_vocab.txt', en_vocab)\n",
        "\n",
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy())\n",
        "\n",
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)\n",
        "\n",
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n",
        "\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}