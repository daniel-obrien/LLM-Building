{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from colorama import Fore, Style\n",
        "from IPython.core.display import HTML"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-05-02 11:09:54.586654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-05-02 11:09:54.854218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2024-05-02 11:09:54.854248: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2024-05-02 11:09:55.908302: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2024-05-02 11:09:55.908401: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2024-05-02 11:09:55.908411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1714648197245
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary_size=5000,\n",
        "        max_sentence_len=50,\n",
        "        embedding_size=256,\n",
        "        n_encoder_decoder_blocks=1,\n",
        "        n_attention_heads=8,\n",
        "        n_units_dense=256,\n",
        "        dropout_rate=0.2,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_sentence_len = max_sentence_len\n",
        "\n",
        "        self.vectorization_en = layers.TextVectorization(\n",
        "            vocabulary_size, output_sequence_length=max_sentence_len\n",
        "        )\n",
        "        self.vectorization_tr = layers.TextVectorization(\n",
        "            vocabulary_size, output_sequence_length=max_sentence_len\n",
        "        )\n",
        "        self.encoder_embedding = layers.Embedding(\n",
        "            vocabulary_size, embedding_size, mask_zero=True\n",
        "        )\n",
        "        self.decoder_embedding = layers.Embedding(\n",
        "            vocabulary_size, embedding_size, mask_zero=True\n",
        "        )\n",
        "        self.positional_encoding = PositionalEncoding(max_sentence_len, embedding_size)\n",
        "        self.encoder_blocks = [\n",
        "            Encoder(embedding_size, n_attention_heads, n_units_dense, dropout_rate)\n",
        "            for _ in range(n_encoder_decoder_blocks)\n",
        "        ]\n",
        "        self.decoder_blocks = [\n",
        "            Decoder(embedding_size, n_attention_heads, n_units_dense, dropout_rate)\n",
        "            for _ in range(n_encoder_decoder_blocks)\n",
        "        ]\n",
        "        self.output_layer = layers.Dense(vocabulary_size, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "\n",
        "        encoder_input_ids = self.vectorization_en(encoder_inputs)\n",
        "        decoder_input_ids = self.vectorization_tr(decoder_inputs)\n",
        "\n",
        "        encoder_embeddings = self.encoder_embedding(encoder_input_ids)\n",
        "        decoder_embeddings = self.decoder_embedding(decoder_input_ids)\n",
        "\n",
        "        encoder_pos_embeddings = self.positional_encoding(encoder_embeddings)\n",
        "        decoder_pos_embeddings = self.positional_encoding(decoder_embeddings)\n",
        "\n",
        "        encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "        decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
        "\n",
        "        # From original paper: \"This masking, combined with fact that the output\n",
        "        # embeddings are offset by one position, ensures that the predictions for\n",
        "        # position i can depend only on the known outputs at positions less than i.\"\n",
        "        batch_max_len_decoder = tf.shape(decoder_embeddings)[1]\n",
        "        decoder_causal_mask = tf.linalg.band_part(  # Lower triangular matrix.\n",
        "            tf.ones((batch_max_len_decoder, batch_max_len_decoder), tf.bool), -1, 0\n",
        "        )\n",
        "        decoder_mask = decoder_causal_mask & decoder_pad_mask\n",
        "\n",
        "        Z = encoder_pos_embeddings\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            Z = encoder_block(Z, mask=encoder_pad_mask)\n",
        "\n",
        "        encoder_output = Z\n",
        "        Z = decoder_pos_embeddings\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            Z = decoder_block(\n",
        "                [Z, encoder_output], mask=[decoder_mask, encoder_pad_mask]\n",
        "            )\n",
        "\n",
        "        return self.output_layer(Z)\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648218049
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adapt_compile(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    valid_dataset,\n",
        "    n_epochs=25,\n",
        "    n_patience=5,\n",
        "    init_lr=0.001,\n",
        "    lr_decay_rate=0.1,\n",
        "    colorama_verbose=False,\n",
        "):\n",
        "    \"\"\"Takes the model vectorization layers and adapts them to the training data.\n",
        "    Then, it prepares the final datasets vectorizing targets and prefetching,\n",
        "    and finally trains the given model. Additionally, provides learning rate scheduling\n",
        "    (exponential decay), early stopping and colorama verbose.\"\"\"\n",
        "\n",
        "    model.vectorization_en.adapt(\n",
        "        train_dataset.map(\n",
        "            lambda sentences, target: sentences[0],  # English sentences.\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "        )\n",
        "    )\n",
        "    model.vectorization_tr.adapt(\n",
        "        train_dataset.map(\n",
        "            lambda sentences, target: sentences[1] + b\" endofseq\",  # Turkish sentences.\n",
        "            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    train_dataset_prepared = train_dataset.map(\n",
        "        lambda sentences, target: (sentences, model.vectorization_tr(target)),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    valid_dataset_prepared = valid_dataset.map(\n",
        "        lambda sentences, target: (sentences, model.vectorization_tr(target)),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\", patience=n_patience, restore_best_weights=True\n",
        "    )\n",
        "    \n",
        "    # The line below doesn't work with multi-file interleaving.\n",
        "    # n_decay_steps = n_epochs * train_dataset_prepared.cardinality().numpy()\n",
        "    # Less elegant solution.\n",
        "    n_decay_steps = n_epochs * len(list(train_dataset_prepared))\n",
        "    scheduled_lr = keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=init_lr,\n",
        "        decay_steps=n_decay_steps,\n",
        "        decay_rate=lr_decay_rate,\n",
        "    )\n",
        "\n",
        "    model_callbacks = [early_stopping_cb]\n",
        "    verbose_level = 1\n",
        "    if colorama_verbose:\n",
        "        model_callbacks.append(ColoramaVerbose())\n",
        "        verbose_level = 0\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=scheduled_lr),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648235766
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "easy_dataset_path = Path(\"/home/azureuser/cloudfiles/code/Users/NAZAR.KHOKHLA.2023/combined4.csv\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648448924
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "easy_dataset = pd.read_csv(easy_dataset_path, encoding=\"utf-8\", engine=\"pyarrow\")\n",
        "easy_dataset = easy_dataset.sample(len(easy_dataset), random_state=42)\n",
        "easy_dataset.head()\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "                                                  English  \\\n506215  Jane stopped collecting teddies at the age of 20.   \n258912                   I know you're afraid of heights.   \n317197               Did you have another fight with Tom?   \n481754                         I like this. I'll take it.   \n344523             I didn't think Tom would let me drive.   \n\n                                       The Other Language  \n506215  Jane 20 yaşındayken oyuncak ayı toplamayı durd...  \n258912              Yüksekliklerden korktuğunu biliyorum.  \n317197                    Tom'la bir daha kavga ettin mi?  \n481754                 Bundan hoşlanıyorum. Onu alacağım.  \n344523   Tom'un araba sürmeme izin vereceğini düşünmedim.  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>The Other Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>506215</th>\n      <td>Jane stopped collecting teddies at the age of 20.</td>\n      <td>Jane 20 yaşındayken oyuncak ayı toplamayı durd...</td>\n    </tr>\n    <tr>\n      <th>258912</th>\n      <td>I know you're afraid of heights.</td>\n      <td>Yüksekliklerden korktuğunu biliyorum.</td>\n    </tr>\n    <tr>\n      <th>317197</th>\n      <td>Did you have another fight with Tom?</td>\n      <td>Tom'la bir daha kavga ettin mi?</td>\n    </tr>\n    <tr>\n      <th>481754</th>\n      <td>I like this. I'll take it.</td>\n      <td>Bundan hoşlanıyorum. Onu alacağım.</td>\n    </tr>\n    <tr>\n      <th>344523</th>\n      <td>I didn't think Tom would let me drive.</td>\n      <td>Tom'un araba sürmeme izin vereceğini düşünmedim.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648463472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_en = easy_dataset[\"English\"].to_numpy()\n",
        "sentences_tr = easy_dataset[\"The Other Language\"].to_numpy()\n",
        "\n",
        "valid_fraction = 0.1\n",
        "valid_len = int(valid_fraction * len(easy_dataset))\n",
        "\n",
        "sentences_en_train = sentences_en[:-valid_len]\n",
        "sentences_tr_train = sentences_tr[:-valid_len]\n",
        "\n",
        "sentences_en_valid = sentences_en[-valid_len:]\n",
        "sentences_tr_valid = sentences_tr[-valid_len:]\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648478767
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input_and_target(sentences_en, sentences_tr):\n",
        "    \"\"\"Return data in the format: `((encoder_input, decoder_input), target)`\"\"\"\n",
        "    return (sentences_en, b\"startofseq \" + sentences_tr), sentences_tr + b\" endofseq\"\n",
        "\n",
        "\n",
        "def from_sentences_dataset(\n",
        "    sentences_en,\n",
        "    sentences_tr,\n",
        "    batch_size=32,\n",
        "    cache=True,\n",
        "    shuffle=False,\n",
        "    shuffle_buffer_size=10_000,\n",
        "    seed=None,\n",
        "):\n",
        "    \"\"\"Creates `TensorFlow` dataset for encoder-decoder RNN from given sentences.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((sentences_en, sentences_tr))\n",
        "    dataset = dataset.map(prepare_input_and_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if cache:\n",
        "        dataset = dataset.cache()\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
        "    return dataset.batch(batch_size)\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648498873
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "easy_train_ds = from_sentences_dataset(\n",
        "    sentences_en_train, sentences_tr_train, shuffle=True, seed=42\n",
        ")\n",
        "easy_valid_ds = from_sentences_dataset(sentences_en_valid, sentences_tr_valid)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648531269
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(\n",
        "        self, max_sentence_len=50, embedding_size=256, dtype=tf.float32, **kwargs\n",
        "    ):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if not embedding_size % 2 == 0:\n",
        "            raise ValueError(\"The `embedding_size` must be even.\")\n",
        "\n",
        "        p, i = np.meshgrid(np.arange(max_sentence_len), np.arange(embedding_size // 2))\n",
        "        pos_emb = np.empty((1, max_sentence_len, embedding_size))\n",
        "        pos_emb[:, :, 0::2] = np.sin(p / 10_000 ** (2 * i / embedding_size)).T\n",
        "        pos_emb[:, :, 1::2] = np.cos(p / 10_000 ** (2 * i / embedding_size)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.positional_embedding[:, :batch_max_length]\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648558765
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size=256,\n",
        "        n_attention_heads=8,\n",
        "        n_units_dense=256,\n",
        "        dropout_rate=0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(\n",
        "            n_attention_heads, embedding_size, dropout=dropout_rate\n",
        "        )\n",
        "        self.feed_forward = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(\n",
        "                    n_units_dense, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
        "                ),\n",
        "                layers.Dense(embedding_size, kernel_initializer=\"he_normal\"),\n",
        "                layers.Dropout(dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.add = layers.Add()\n",
        "        self.normalization = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        Z = inputs\n",
        "        skip_Z = Z\n",
        "        Z = self.multi_head_attention(Z, value=Z, attention_mask=mask)\n",
        "        Z = self.normalization(self.add([Z, skip_Z]))\n",
        "        skip_Z = Z\n",
        "        Z = self.feed_forward(Z)\n",
        "        return self.normalization(self.add([Z, skip_Z]))\n",
        "\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size=256,\n",
        "        n_attention_heads=8,\n",
        "        n_units_dense=256,\n",
        "        dropout_rate=0.2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.masked_multi_head_attention = layers.MultiHeadAttention(\n",
        "            n_attention_heads, embedding_size, dropout=dropout_rate\n",
        "        )\n",
        "        self.multi_head_attention = layers.MultiHeadAttention(\n",
        "            n_attention_heads, embedding_size, dropout=dropout_rate\n",
        "        )\n",
        "        self.feed_forward = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(\n",
        "                    n_units_dense, activation=\"relu\", kernel_initializer=\"he_normal\"\n",
        "                ),\n",
        "                layers.Dense(embedding_size, kernel_initializer=\"he_normal\"),\n",
        "                layers.Dropout(dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.add = layers.Add()\n",
        "        self.normalization = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        decoder_mask, encoder_mask = mask  # type: ignore\n",
        "        Z, encoder_output = inputs\n",
        "        Z_skip = Z\n",
        "        Z = self.masked_multi_head_attention(Z, value=Z, attention_mask=decoder_mask)\n",
        "        Z = self.normalization(self.add([Z, Z_skip]))\n",
        "        Z_skip = Z\n",
        "        Z = self.multi_head_attention(\n",
        "            Z, value=encoder_output, attention_mask=encoder_mask\n",
        "        )\n",
        "        Z = self.normalization(self.add([Z, Z_skip]))\n",
        "        Z_skip = Z\n",
        "        Z = self.feed_forward(Z)\n",
        "        return self.normalization(self.add([Z, Z_skip]))\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648576725
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLR = (Style.BRIGHT + Fore.WHITE)\n",
        "RED = Style.BRIGHT + Fore.RED\n",
        "class ColoramaVerbose(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\n",
        "            f\"{CLR}Epoch: {RED}{epoch + 1:02d}{CLR} -\",\n",
        "            f\"{CLR}loss: {RED}{logs['loss']:.5f}{CLR} -\",\n",
        "            f\"{CLR}accuracy: {RED}{logs['accuracy']:.5f}{CLR} -\",\n",
        "            f\"{CLR}val_loss: {RED}{logs['val_loss']:.5f}{CLR} -\",\n",
        "            f\"{CLR}val_accuracy: {RED}{logs['val_accuracy']:.5f}\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648653895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_new = Transformer(max_sentence_len=15)\n",
        "transformer_new_history = adapt_compile(\n",
        "    transformer_new, easy_train_ds, easy_valid_ds, colorama_verbose=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648688698
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_new.load_weights('model_weights')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f0fd39ab940>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648723724
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(model.max_sentence_len):\n",
        "        X_encoder = np.array([sentence_en])\n",
        "        X_decoder = np.array([\"startofseq \" + translation])\n",
        "        # Last token's probas.\n",
        "        y_proba = model.predict((X_encoder, X_decoder), verbose=0)[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = model.vectorization_tr.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()\n"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648743925
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation4 = translate(transformer_new_history, \"She ordered him to do it.\")"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648755722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(translation4)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "o ona bunu yapmasını verdi\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714648771992
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}